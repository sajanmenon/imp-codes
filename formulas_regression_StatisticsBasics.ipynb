{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression assumptions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. No Autocorrelation     : Durbin Watson test : ACF Plot\n",
    "2. No heteroscedasticity  : Breush Test, Goldfeld Test  : Residual vs Fittes\n",
    "3. No Multicollinearity   : VIF  & Correlation matrix   \n",
    "4. Normality of Residuals : Jargue -Bera, Shapiro-wilk Test: Normal QQ\n",
    "5. Linearity of Residuals : scatter plots and Rainbow Test. \n",
    "\n",
    "6. target should be numeric and continous\n",
    "7. theres should be 2 or more independent variables which are numeric/categoric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.no autocorrelation\n",
    "(Test needed : Durbin- Watson Test) value b/w 0-4, 0-2 +ve autocorrelation and good for model.\n",
    "                                             2-4 -ve autocorrelation and bad for model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is acf plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.tsa.api as smt   ##change lags as per the number of model_name.resid (print separately and see)\n",
    "\n",
    "acf = smt.graphics.plot_acf(model_name.resid, lags=40 , alpha=0.05)\n",
    "acf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.normality of residuals. \n",
    "                        (test needed: jarque-bera) \n",
    "                        The critical chi square value at the 5% level of significance is 5.99. \n",
    "                        H0: residuals are normally distributed\n",
    "                        \n",
    "                        If the computed value exceeds this value the null hypothesis is rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats  #gives jarque-bera statistic with its pvalue.\n",
    "print(stats.jarque_bera(model_name.resid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(model_name.resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Linearity of residuals.\n",
    "                 : there should be linear relation bw each of the idv with dv.\n",
    "                 we have two options here:\n",
    "                 opt1:scatter plot the observed values Vs predicted values and scatter plot the Residual Vs predicted values.\n",
    "                       first plot should be linear and in second, points hsould be distributed around a horizontal line\n",
    "                  opt2: Rainbow test:\n",
    "                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# opt 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.api as sms\n",
    "sns.set_style('darkgrid')\n",
    "sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "\n",
    "def linearity_test(model, y):\n",
    "    '''\n",
    "    Function for visually inspecting the assumption of linearity in a linear regression model.\n",
    "    It plots observed vs. predicted values and residuals vs. predicted values.\n",
    "    \n",
    "    Args:\n",
    "    * model - fitted OLS model from statsmodels\n",
    "    * y - observed values\n",
    "    '''\n",
    "    fitted_vals = model.predict()\n",
    "    resids = model.resid\n",
    "\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    \n",
    "    sns.regplot(x=fitted_vals, y=y, lowess=True, ax=ax[0], line_kws={'color': 'red'})\n",
    "    ax[0].set_title('Observed vs. Predicted Values', fontsize=16)\n",
    "    ax[0].set(xlabel='Predicted', ylabel='Observed')\n",
    "\n",
    "    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[1], line_kws={'color': 'red'})\n",
    "    ax[1].set_title('Residuals vs. Predicted Values', fontsize=16)\n",
    "    ax[1].set(xlabel='Predicted', ylabel='Residuals')\n",
    "    \n",
    "linearity_test(lin_reg, y)    ##give y_train_name['column_name'] here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# opt 2: rainbow test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "sm.stats.diagnostic.linear_rainbow(res=lin_reg, frac=0.5)  #line_reg is model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import pylab\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "st_residual = lin_reg.get_influence().resid_studentized_internal     ##line_reg is the model_name\n",
    "stats.probplot(st_residual, dist=\"norm\", plot = pylab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPECTATION : THE MEAN VALUE OF THE RESIDUALS SHOULD BE ZERO.\n",
    "    LET's CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.resid.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 -  Homoscedasticity_test(using goldfeld test) OR (Beusch-Wagon Test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test is based on the hytpothesis testing where null and alternate hypothesis are:  (ui is the residuals)\n",
    "$$ H_{0} : \\sigma_{u_{i}}~is~constant~across~the~range~of~data $$\n",
    "\n",
    "$$ H_{a} : \\sigma_{u_{i}}~is~not~constant~across~the~range~of~data $$\n",
    "\n",
    "The residuals should be homoscedastic.\n",
    "\n",
    "looking at the pvalue , we can reject/accept the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.compat import lzip\n",
    "import numpy as np\n",
    "from statsmodels.compat import lzip\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.api as sms\n",
    "sns.set_style('darkgrid')\n",
    "sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "\n",
    "model = lin_reg\n",
    "fitted_vals = model.predict()\n",
    "resids = model.resid\n",
    "resids_standardized = model.get_influence().resid_studentized_internal\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[0], line_kws={'color': 'red'})\n",
    "ax[0].set_title('Residuals vs Fitted', fontsize=16)\n",
    "ax[0].set(xlabel='Fitted Values', ylabel='Residuals')\n",
    "sns.regplot(x=fitted_vals, y=np.sqrt(np.abs(resids_standardized)), lowess=True, ax=ax[1], line_kws={'color': 'red'})\n",
    "ax[1].set_title('Scale-Location', fontsize=16)\n",
    "ax[1].set(xlabel='Fitted Values', ylabel='sqrt(abs(Residuals))')\n",
    "\n",
    "name = ['F statistic', 'p-value']\n",
    "test = sms.het_goldfeldquandt(model.resid, model.model.exog)\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. No multicollinearity\n",
    "            we check vif values to evaluate multicollinearity and also heatmap\n",
    "            delete features with vif>4 one by one and build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\n",
    "pd.DataFrame({'vif': vif[0:]}, index=X.columns).T   ##change 0 into 1 if there is no 'const' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##vif will take one idv as target and find raltion with other features. and it will do the same with all features.\n",
    "whichevr has multicollinearity , r2 will be high and vif score will be high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmapmap(bos.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(bos.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. \n",
    "## for sqrt(X)\n",
    "df2 = bos.copy()\n",
    "final_df = df2.transform(lambda x: x**0.5)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.\n",
    "#for log  ( if data has '0', it will be infinity after log, which is not possible.)\n",
    "df2 = bos.copy()\n",
    "df_final1 = df2.transform(lambda X: np.log(X))\n",
    "df_final1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.\n",
    "# for inverse ( if data has categorical values, 0 will give infinity after transformation, which is not possible)\n",
    "df_final1 = df2.transform(lambda X: 1/X)\n",
    "df_final1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.\n",
    "# for exponential  (check for infinity)\n",
    "df2 = bos.copy()\n",
    "df_final1 = df2.transform(lambda X: np.exp(X))\n",
    "df_final1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5.\n",
    "only transform target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.using pearson correlation value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "cor_target = abs(cor[\"Price\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.5]\n",
    "relevant_features    ##check multicollinearity now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. wrapper method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 backward elimination  ( gives significant column names only as o/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(x_1_train.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    X_1 = x_1_train[cols]\n",
    "    X_1 = sm.add_constant(X_1)                                    ##if constant is added already, remove this.\n",
    "    model = sm.OLS(y_1_train,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[0:],index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "selected_features_BE = cols\n",
    "print(selected_features_BE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmax=1\n",
    "cols = list(Xp.columns)\n",
    "X_1 = Xp.copy()\n",
    "\n",
    "while (len(cols)>0):\n",
    "    p = []\n",
    "    X_1 = X_1[cols]\n",
    "    model = sm.OLS(ys, X_1).fit()\n",
    "    p = model.pvalues\n",
    "    pmax = max(p)\n",
    "    \n",
    "    if (pmax>0.05):\n",
    "        cols = list(p.drop(p[p==pmax].index).index)\n",
    "        #print(cols)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "selected = cols\n",
    "print(selected)\n",
    "print(len(selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 step foraward selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RF classifier to use in feature selection\n",
    "clf = LinearRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "# Build step forward feature selection\n",
    "sfs1 = sfs(clf,k_features = 10,forward=True,\n",
    "           floating=False, scoring='r2',\n",
    "           verbose=2,\n",
    "           cv=5)   #cv is the \n",
    "\n",
    "# Perform SFFS\n",
    "sfs1 = sfs1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which features?\n",
    "feat_cols = list(sfs1.k_feature_idx_)\n",
    "print(feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 RFE  Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing RFE model\n",
    "rfe = RFE(model, 11)  ##take 11 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X,y)    ##x1_train, y1_train\n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)                #y1_train\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### another method to try-Recursive feature Elemination and Grid Search Cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "model = LinearRegression()\n",
    "selector = RFECV(model,cv=5)\n",
    "selector = selector.fit(X, y)\n",
    "print(f\"Out of {len(X.columns)} features, best number of features {selector.n_features_}\")\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score\")\n",
    "plt.plot(range(1, len(X.columns) + 1), selector.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "selector = RFECV(model,cv=5)\n",
    "selector = selector.fit(X, y)\n",
    "print(f\"Out of {len(X.columns)} features, best number of features {selector.n_features_}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score\")\n",
    "plt.plot(range(1, len(X.columns) + 1), selector.grid_scores_)\n",
    "print(X.columns[selector.support_].values)                         ##build another model with these selected features only.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Embedded Method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Ridge regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# higher the alpha value, more restriction on the coefficients; \n",
    "# low alpha > more generalization, coefficients are barely\n",
    "rr = Ridge(alpha=0.01) \n",
    "# restricted and in this case linear and ridge regression resembles\n",
    "rr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr1000=Ridge(alpha=1000,max_iter=10e5)\n",
    "rr1000.fit(x_train_base,y_train_base)\n",
    "ridge_train_score=rr1000.score(x_train_base,y_train_base)\n",
    "ridge_test_score=rr1000.score(x_test_base,y_test_base)\n",
    "coeff_used= np.sum(rr1000.coef_!=0)\n",
    "print(train_score)\n",
    "print(test_score)\n",
    "print(coeff_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge_train_score = rr.score(X_train,y_train)\n",
    "Ridge_test_score = rr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rr.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Ridge; $\\alpha = 0.01$',zorder=7) # zorder for ordering the markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rr100.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Ridge; $\\alpha = 100$') # alpha here is for transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### or use hypertuning by random or grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sm.add_constant(x)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet\n",
    "lr=LinearRegression()\n",
    "ridge=Ridge()\n",
    "r_params={'alpha':np.arange(0.01,1,0.01)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rs_ridge=RandomizedSearchCV(ridge,param_distributions=r_params)\n",
    "rs_ridge.fit(x_train,y_train)\n",
    "rs_ridge.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge=Ridge(**rs_ridge.best_params_)\n",
    "ridge.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_train_rsquare=ridge.score(x_train,y_train)   #rsquare value\n",
    "ridge_test_rsquare=ridge.score(x_test,y_test)\n",
    "print(ridge_train_rsquare)\n",
    "print(ridge_test_rsquare)\n",
    "\n",
    "y_pred_train=ridge.predict(x_train) \n",
    "y_pred_test=ridge.predict(x_test)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "rmse_ridge_train=np.sqrt(metrics.mean_squared_error(y_train,y_pred_train))  #for rmse value from train data.\n",
    "rmse_ridge_test=np.sqrt(metrics.mean_squared_error(y_test,y_pred_test))\n",
    "\n",
    "print(rmse_ridge_train)\n",
    "print(rmse_ridge_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "lasso.fit(X_train,y_train)\n",
    "train_score=lasso.score(X_train,y_train)\n",
    "test_score=lasso.score(X_test,y_test)\n",
    "coeff_used = np.sum(lasso.coef_!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of coeef used\", coeffecient_used )\n",
    "print('r2 score on train data',train_score )   \n",
    "print('r2 score on test data',test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso001 = Lasso(alpha=0.01, max_iter=10e5)\n",
    "lasso001.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7) # alpha here is for transparency\n",
    "plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\\alpha = 0.01$') # alpha here is for transparency\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,2)\n",
    "plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7) # alpha here is for transparency\n",
    "plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\\alpha = 0.01$') # alpha here is for transparency\n",
    "plt.plot(lasso00001.coef_,alpha=0.8,linestyle='none',marker='v',markersize=6,color='black',label=r'Lasso; $\\alpha = 0.00001$') # alpha here is for transparency\n",
    "plt.plot(lr.coef_,alpha=0.7,linestyle='none',marker='o',markersize=5,color='green',label='Linear Regression',zorder=2)\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc=4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### or use hypertuning by randomsearch or gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sm.add_constant(x)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet\n",
    "lr=LinearRegression()\n",
    "lasso=Lasso()\n",
    "l_params={'alpha':np.arange(0.01,1,0.01)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rs_lasso=RandomizedSearchCV(lasso,param_distributions=l_params)\n",
    "rs_lasso.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rs_lasso.best_params_)\n",
    "lasso=Lasso(**rs_lasso.best_params_)\n",
    "lasso.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_train_rsquare=lasso.score(x_train,y_train)\n",
    "print(lasso_train_rsquare)\n",
    "lasso_test_rsquare=lasso.score(x_test,y_test)\n",
    "print(lasso_test_rsquare)\n",
    "\n",
    "y_pred_train=lasso.predict(x_train)\n",
    "y_pred_test=lasso.predict(x_test)\n",
    "\n",
    "rmse_lasso_train=np.sqrt(metrics.mean_squared_error(y_train,y_pred_train))\n",
    "print(rmse_lasso_train)\n",
    "rmse_lasso_test=np.sqrt(metrics.mean_squared_error(y_test,y_pred_test))\n",
    "print(rmse_lasso_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Elastic Net regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's perform a cross-validation to find the best combination of alpha and l1_ratio\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "from sklearn.metrics import r2_score\n",
    "## if you put l1_ratio as 0 it is lasso, 1 for ridge. there is not use though as we can directly do lasso and ridge\n",
    "## n_jobs=-1 should be given always as it allows full spec of our system to be used.\n",
    "cv_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, .995, 1], eps=0.001, n_alphas=100, fit_intercept=True, \n",
    "                        normalize=True, precompute='auto', max_iter=2000, tol=0.0001, cv=5, \n",
    "                        copy_X=True, verbose=0, n_jobs=-1, positive=False, random_state=None, selection='cyclic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal alpha: %.8f'%cv_model.alpha_)\n",
    "print('Optimal l1_ratio: %.3f'%cv_model.l1_ratio_)\n",
    "print('Number of iterations %d'%cv_model.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with best parameters from CV\n",
    "model = ElasticNet(l1_ratio=cv_model.l1_ratio_, alpha = cv_model.alpha_, max_iter=cv_model.n_iter_, fit_intercept=True, normalize = True)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(y_train, model.predict(X_train)))# training data performance\n",
    "print(r2_score(y_test, model.predict(X_test))) # test data performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### or use hypertuning with randomsearch or grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet\n",
    "lr=LinearRegression()\n",
    "en=ElasticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_params={'alpha':np.arange(0.01,1,0.01),'l1_ratio':np.arange(0.1,1,0.01)}\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rs_en=RandomizedSearchCV(en,param_distributions=en_params)\n",
    "rs_en.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_en.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en=ElasticNet(**rs_en.best_params_)\n",
    "en.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train_rsquare=en.score(x_train,y_train)   #rsquare value\n",
    "en_test_rsquare=en.score(x_test,y_test)\n",
    "print(en_train_rsquare)\n",
    "print(en_test_rsquare)\n",
    "\n",
    "y_pred_train=en.predict(x_train)\n",
    "y_pred_test=en.predict(x_test)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "rmse_en_train=np.sqrt(metrics.mean_squared_error(y_train,y_pred_train))  #for rmse value from train data.\n",
    "rmse_en_test=np.sqrt(metrics.mean_squared_error(y_test,y_pred_test))\n",
    "\n",
    "print(rmse_en_train)\n",
    "print(rmse_en_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to make dataframe of all results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame({ 'RMSE':[rmse_lasso_train,rmse_lasso_test,rmse_ridge_train,rmse_ridge_test,rmse_en_train,rmse_en_test],\n",
    "                    'R square value' : [lasso_train_rsquare,lasso_test_rsquare,ridge_train_rsquare,ridge_test_rsquare,\n",
    "                     en_train_rsquare,en_test_rsquare]},index=['lasso_train','lasso_test','ridge_train','ridge_test','ElasticNet_train','ElasticNet_train'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# general libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city=pd.read_csv('C:/Users/SAJAN P MENON/Desktop/week-6/mini project/bigcity.csv',index_col='Unnamed: 0')   ## change the 1st column of dataframe into indexcolumn\n",
    "city.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for no. and percentage of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_missing_values =df.columns[df.isnull().any()]       \n",
    "df[columns_with_missing_values].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "           # To hold variable names\n",
    "labels = []\n",
    "           # To hold the count of missing values for each variable\n",
    "valuecount = []\n",
    "\n",
    "# To hold the percentage of missing values for each variable\n",
    "percentcount = []\n",
    "\t\t\t\t\t\n",
    "for col in columns_with_missing_values:\n",
    "    labels.append(col)\n",
    "    valuecount.append(df[col].isnull().sum())     ##df is the main dataframe with null values\n",
    "       # housepricesdata.shape[0] will give the total row count\n",
    "\n",
    "    percentcount.append(df[col].isnull().sum()/df.shape[0])    ##df is the main dataframe with null values\n",
    "    ind = np.arange(len(labels))\n",
    "\t\t\t\t\t\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(20,18))\n",
    "\n",
    "\t\t\t\t\t\n",
    "rects = ax1.barh(ind, np.array(valuecount), color='blue')\n",
    "ax1.set_yticks(ind)\n",
    "ax1.set_yticklabels(labels, rotation='horizontal')\n",
    "ax1.set_xlabel(\"Count of missing values\")\n",
    "ax1.set_title(\"Variables with missing values\")\n",
    "\t\t\t\t\n",
    "rects = ax2.barh(ind, np.array(percentcount), color='pink')\n",
    "ax2.set_yticks(ind)\n",
    "ax2.set_yticklabels(labels, rotation='horizontal')\n",
    "ax2.set_xlabel(\"Percentage of missing values\")\n",
    "ax2.set_title(\"Variables with missing values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is for analysing whether there are multiple null values in same row at same time.\n",
    "#should be done on dataset without replacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import seaborn. We will use seaborn to generate our charts\n",
    "import seaborn as sns\n",
    "\n",
    " \t\t\t\t\t\t\t\n",
    "           # We will import matplotlib to resize our plot figure\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    " \t\t\t\t\t\t\t\n",
    "           # cubehelix palette is a part of seaborn that produces a colormap\n",
    "cmap = sns.cubehelix_palette(light=1, as_cmap=True, reverse=True)\n",
    "sns.heatmap(df.isnull(), cmap=cmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for selecting only required dtype columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_int=auto.select_dtypes(include=['float64','int64'])\n",
    "auto_int.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for plotting box plot for numerical columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1=aut.columns\n",
    "\n",
    "for i in range (0,len(col1)):\n",
    "    if aut[col1[i]].dtypes=='int64' or aut[col1[i]].dtypes=='float64':\n",
    "        sns.boxplot(y=auto[col1[i]])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# statistical tests for dropping features :\n",
    "-we cannot blindly drop features by taking inference from correalation matrix. following tests are used for that:\n",
    "-all these tests have H0 and H1: for dropping columns, all should prove h0 as false. (that is accept H1)\n",
    "Parametric Tests:\n",
    "1)stats.shapiro(df['house_age'])\n",
    "2)stats.levene(df['house_age'],df['house_price'])  #house_price is the target variable and house_age is IDV and needs to be                                                          dropped.\n",
    "Non-Parametric Tests:\n",
    "1)stats.mannwhitneyu(df['transaction_date'],df['house_price'])  #transaction_date is the IDV and needs to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression model - statsmodel/statistics way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import statsmodels.api as sm\n",
    "X_constant = sm.add_constant(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_constant,y, test_size = 0.30, random_state = 1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "lin_reg = sm.OLS(y_train,x_train).fit()\n",
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression model - sklearn/machine learning way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "regressor=LinearRegression()\n",
    "regressor.fit(x1_train,y1_train)\n",
    "regressor.score(x1_train,y1_train)  #for r square value\n",
    "regressor.coef_  #for coeffecient value\n",
    "regressor.intercept_  ##for intercept value\n",
    "\n",
    "y_pred=regressor.predict(x1_test)\n",
    "rmse=np.sqrt(metrics.mean_squared_error(y1_test,y_pred))\n",
    "mae=metrics.mean_absolute_error(y_pred,y_test)\n",
    "mse=metrics.mean_squared_error(y_pred,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for mean absolute percentage error-mape and rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "print('MAPE :',mean_absolute_percentage_error(y,model.predict(X)))\n",
    "print('RMSE :',np.sqrt(mean_squared_error(y, model.predict(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different graphs/plot avilable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. univariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    numerical:\n",
    "        1)distribution plots\n",
    "        2)kde plot\n",
    "        3)histogram\n",
    "        4)boxplot(can be done for multivariant)\n",
    "    categorical:\n",
    "        1)bar chart (if you have multiple categories to plot, pass it as a list)\n",
    "        2)pie chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Bivariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    numerical vs numerical: \n",
    "        1)lineplot\n",
    "        2)scatter plot\n",
    "        3)regplot\n",
    "    categorical vs categorical: (can apply hue)\n",
    "        1) count plot\n",
    "        2)stacked bar chart (easy by making cross tab and applying.plot method)(cor1.plot(kind='bar',stacked=True)\n",
    "        3)grouped bar chart\n",
    "        4)segmented bar chart\n",
    "    categorical vs numerical:\n",
    "        1)boxplot\n",
    "        2)violin plot\n",
    "        3)grouped kde\n",
    "        4)bar chart (on sumamry statistics, valuecount)\n",
    "        5)strip plot\n",
    "        6)swarm plot  (superimpose it with boxplots for inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab:\n",
    "\n",
    "    df1.groupby(['sex','day']).agg({'tip': 'max' })\n",
    "    pd.crosstab(df1['sex'],df1['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for drawing heatmap with mask. upper part wont come."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heat_map(dff):\n",
    "    corrs = dff.corr()\n",
    "\n",
    "    # Set the figure size\n",
    "    fig, ax = plt.subplots(figsize=(20,18))\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corrs, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Plot the heatmap\n",
    "    ax = sns.heatmap(corrs, mask=mask, annot=True)\n",
    "\n",
    "    # Resize the labels\n",
    "    ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=14, rotation=90)\n",
    "    ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=14, rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_heat_map(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# statistical methods for decission making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Discrete probability Distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.a) binomial distribution: (yes/no)\n",
    "-it is the probability of getting x number of successes in n number of trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)=(n!/(n-x)! * x!) * p power x * q power (n-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    import scipy.stats as stats\n",
    "    stats.binom.pmf(k=3,n=2,p=.868)        # (p=prob of success, n=total no of trails, k= no of success in n trials)\n",
    "    stats.binom.cdf(k=2,n=3,p=.868)        # the value shown in k will will included in cdf. means, probability of k=2 is also \n",
    "                                               included.\n",
    "    stats.binom.sf(1,3,0.868)              # gives same output as (stats.binom.pmf(3,3,0.868)+stats.binom.pmf(2,3,0.868))\n",
    "    \n",
    "    \n",
    "    mean=n*p (mean is called as expected value)\n",
    "    variance=n*p*q\n",
    "    std=np.sqrt(n*p*(1-p))\n",
    "    \n",
    "    # for plotting binomial distribution\n",
    "    n=3\n",
    "    k=np.arange(0,4)\n",
    "    p=0.868\n",
    "    binomial=stats.binom.pmf(k,n,p)\n",
    "    plt.plot(k,binomial,'o-')\n",
    "    plt.title('binomial')\n",
    "    plt.xlabel('number of orders filled correctly')\n",
    "    plt.ylabel('prob of orders filled correctly')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.b). poisson distribution: \n",
    "-probability of number of ocurrences over a specified interval of time or other units\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(x)= (mean power x) * (e power (-mean)) / x!\n",
    "\n",
    "mean= average/mean/expected values of number of occurence\n",
    "e=2.71828\n",
    "x=number of success\n",
    "f(x)=possibility of occurence in an interval\n",
    "\n",
    "std deviation=np.sqrt(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    stats.poisson.pmf(x,mean) #x=number of occurence, mean=average occurence.\n",
    "    stats.poisson.cdf(x,mean)\n",
    "    \n",
    "    \n",
    "    \n",
    "    stats.poisson.pmf(2,3)  #2 is the number of occurence, 3 is the average occurence /interval.\n",
    "    stats.poisson.cdf(4,3)  # number of occurence=4. 4 is also included.\n",
    "    std=np.sqrt(average occurence/interval)\n",
    "    \n",
    "    #for plotting poisson distribution\n",
    "    n=np.arange(0,16)\n",
    "    rate=3\n",
    "    poisson=stats.poisson.pmf(n,rate)\n",
    "    plt.plot(n,poisson, 'o-')\n",
    "    plt.title('Poisson')\n",
    "    plt.xlabel('Number of Work related injuries')\n",
    "    plt.ylabel('Prob of Work related injuries')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. continous probability distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    cdf method:\n",
    "    stats.norm.cdf(z)  ## area under the curve from left is calculated here.\n",
    "    \n",
    "    sf methodL\n",
    "    stats.norm.sf(z) ## area under the curve from right is calculated.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean=mode=median in a perfect normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confidence interval:\n",
    "    1) z_upper=stats.norm.isf(.005)  ## when cofidence interval is 99%. (1%/2 to be taken) #remember .isf calculates from right to left\n",
    "       z_lower=stats.norm.isf(1-.005)\n",
    "    2)  x_lower=(z_lower*(std/np.sqrt(n)))+mean\n",
    "        x_upper=(z_upper*(std/np.sqrt(n)))+mean\n",
    "        \n",
    "    3) (z*std.pop/np.sqrt(sample_size))+ pop.mean or sample_mean\n",
    "        pop.mean or sample_mean-(z*std.pop/np.sqrt(sample_size))\n",
    "    \n",
    "    \n",
    "\n",
    "        or\n",
    "    method 2: lci,uci=stats.norm.interval( .95,avg,s)  # avg can be mean_pop or sample_pop, s can be sigma or sigma/sqrt(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confidence interval: (when total number of trials and number of success only given)\n",
    "import  statsmodels.stats.proportion as  SMP \n",
    "\n",
    "y=total trials\n",
    "x=number of success\n",
    "\n",
    "LCI, UCI    = SMP.proportion_confint(count = x, nobs = y, alpha = z, method = 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How large a sample would be necessary to estimate the population proportion with a margin of error of .05 at 95% confidence? \n",
    "#given, 110 people said yes out of 200 people surveyed.\n",
    "\n",
    "sample_size= np.square((Z@CI)/error_margin)  *  p(true_proportion)  * (1-p)\n",
    "\n",
    "\n",
    "favour = 110\n",
    "total = 200\n",
    "p=favour/total==0.55  (p is true_proportion)\n",
    "ci = 95\n",
    "error=.05\n",
    "z@critical=1.96  (z=stats.norm.isf(.025))\n",
    "\n",
    "\n",
    "sample size= (np.square(z@critical/error))*0.55*(1-0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "company says mileage is 25km/hr\n",
    "h0: mileage is less than 25km/hr\n",
    "\n",
    "h1: mileage is grater than 25km/hr\n",
    "    \n",
    "result: if we get pvalue less than .05, we accept h1. hence it becomes right tail. there are possibility of both one tail/two tail in one sample ttest,two sample paired ttest,two sample independent ttest. as per the h0 declared, it can be right tail or left tail.\n",
    "But, anova and chi square test are only one tail (right tail) as we check whethr there is any difference . we dont check less/more. hence one tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     test       |    dependent variable |  independent variable\n",
    "\n",
    "1.  Anova            continous                 categorical\n",
    "2.  chi_square       categorical               categorical\n",
    "3.  regression        continous                continous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.when only proportions are given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there evidence to conclude that the number of people travelling from Hyderabad to Mumbai is different from the number of people travelling from Mumbai to Delhi in a week.\n",
    "\n",
    "Population 1: Hyderabad to Mumbai \n",
    "        n1 = 1200\n",
    "        x1 = 452\n",
    "        s1 = 12\n",
    "\n",
    "        \n",
    "        \n",
    "Population 2: Mumbai to Delhi \n",
    "        n2 = 800\n",
    "        x2 = 523\n",
    "        s2 = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "x,p_val=proportions_ztest((452,523),(1200,800))\n",
    "x,p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### z test (sample size>30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal z test:\n",
    "z=(mean_sample-mean_population) / (sigma_sample/np.sqrt(total_sample_size))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### sample size<30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.stats             import ttest_1samp,ttest_ind, wilcoxon,ttest_rel\n",
    "from statsmodels.stats.power import ttest_power\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. one sample ttest:\n",
    "     t_statistic, p_value = ttest_1samp(daily_intake, 7600)\n",
    "2. two sample ttest-independent:\n",
    "    t_statistic, p_value  =  stats.ttest_ind(Weight_Male,Weight_Female)\n",
    "    print(p_value)  \n",
    "3. two sample ttest-relative \n",
    "    t_statistic, p_value  =  stats.ttest_rel(wt_after, wt_before )  ##after comes first and then before. always\n",
    "    print(p_value)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 One sample ttest\n",
    "This can be performed on a single continuous variable to check that mean of that population is above or below by how much and what is the interval.\n",
    "eg-Is mean salary of the population greater than 270000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Two sample ttest\n",
    "This test can be performed on one categorical column with another continuous column for checking whether the mean of one sub category issame as other sub-category.\n",
    "\n",
    "example-The mean salary of male is same as mean salary of female or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "2 sample Proportion test\n",
    "This test can be performed on two categorical column\n",
    "To check whether the proportion of one subcategory is same as the others.\n",
    "\n",
    "example- the propotion of male placed is same as proportion of female placed or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 oneway Annova test\n",
    "This can be done on 1 categorical column with two continous variable\n",
    "\n",
    "example- The mean of male salary and marks in degree is same as that of female or not \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Twoway Annova test\n",
    "This can be done on 1 categorical with 2 or more continuous variable.\n",
    "\n",
    "example-The mean of experienced salary and marks in project and degree is same as female or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Annova test: one way  (no need of making dataframe)\n",
    "-we are comparing different categorical groups in column with target varaible. whether the means(wrt target varaible) of these different groups are same or not\n",
    "-example: difference in mean of salary between 3 cities. (3 cities are in same columnn). hence,we can say this is an extension of two sample test. there,we only compare between two categories.\n",
    "\n",
    "import scipy.stats as stats   \n",
    "    \n",
    "life_type_df = pd.DataFrame()\n",
    "\n",
    "df1            = pd.DataFrame({'life_type': 'A', 'life_hours':life_type_A})  ##life_type_A is the sample 1 array\n",
    "df2            = pd.DataFrame({'life_type': 'B', 'life_hours':life_type_B})  ##life_type_B is the sample 2 array\n",
    "df3            = pd.DataFrame({'life_type': 'C', 'life_hours':life_type_C})  ##life_type_C is the sample 3 array\n",
    "\n",
    "life_type_df = life_type_df.append(df1) \n",
    "life_type_df = life_type_df.append(df2)\n",
    "life_type_df = life_type_df.append(df3) \n",
    "\n",
    "stats.f_oneway(life_type_A,life_type_B,life_type_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### post-hoc analysis:\n",
    " to check which group's mean is different:\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "print(pairwise_tukeyhsd(life_type_df['dependant_variable'],life_type_df['independent_varaible'],alpha=0.05))\n",
    "\n",
    "#inference: post-hoc analysis is used to determine which variables has significant difference. anova can only tell whether there is any significant difference or not. upper and lower here will give the range of mean difference of variable A and variable B. as it has 0 in it, we cant reject null hypothesis. hence it is shown as false under \"reject\"\n",
    "\n",
    "anova test says we should reject null hypothesis and post-host analysis says we should accept the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anova test: two way (make dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-we are comparing different categorical groups in column with target varaible. whether the means(wrt target varaible) of these different groups are same or not\n",
    "-here, we compare multiple categorical columns with one numerical columns. in one way anova, we only take 1 categorical column with one numerical column.\n",
    "-remember , null nypothesis say population means of a factor are same.if it gets accepted, it means the said factor has an significant effect on dependenet varaible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The null hypotheses for each of the sets are given below.\n",
    "\n",
    "1) The population means of the first factor (Board_SSC) are equal.\n",
    "2) The population means of the second factor (Course_Degree) are equal.\n",
    "3) There is no interaction between the two factors -Board_SSC and  Course_Degree\n",
    "Alternative Hypothesis:\n",
    "\n",
    "1) The population means of the first factor ((Board_SSC) are not equal.\n",
    "2) The population means of the second factor (Course_Degree) are not equal.\n",
    "3) There is an interaction between the two factors - Board_SSC and  Course_Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0_val  = df1['Brand X'].values\n",
    "d10_val = df1['Brand Y'].values\n",
    "d20_val = df1['Brand Z'].values\n",
    "l_val   = df1['Loc'].values\n",
    "\n",
    "df1   = pd.DataFrame({'Loc': l_val, 'Brand':'X','Qty': d0_val})\n",
    "df2   = pd.DataFrame({'Loc': l_val, 'Brand':'Y','Qty': d10_val})\n",
    "df3   = pd.DataFrame({'Loc': l_val, 'Brand':'Z','Qty': d20_val})\n",
    "\n",
    "Sale_qty_df = pd.DataFrame()\n",
    "\n",
    "Sale_qty_df = Sale_qty_df.append(df1) \n",
    "Sale_qty_df = Sale_qty_df.append(df2) \n",
    "Sale_qty_df = Sale_qty_df.append(df3) \n",
    "\n",
    "pd.DataFrame(Sale_qty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another type of data arrangement:\n",
    "table1  = [['Day','Store-A','Store-B','Store-C','Store-D','Store-E'], [1,69, 71, 64, 67, 56],\\\n",
    "           [2, 68, 76, 79, 87, 76], [3, 71, 77, 74, 84, 72], [4, 70, 73, 71, 78, 73], [5, 60, 64, 67, 79, 58]]\n",
    "\n",
    "\n",
    "headers = table1.pop(0) #\n",
    "\n",
    "df1 = pd.DataFrame(table1, columns=headers)\n",
    "print(df1)\n",
    "\n",
    "df=df1.melt(value_vars=['Store-A','Store-B','Store-C','Store-D','Store-E'],id_vars='Day')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##apply this after making the dataframe.\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "model=ols(formula='value~C(variable)+C(Day)+variable:Day',data=df).fit()  ##c for categorical \n",
    "aov_table=anova_lm(model,typ=2)\n",
    "\n",
    "print(aov_table)\n",
    "\n",
    "# Inference: \n",
    "\n",
    "# From the above table: Pvalue for Loc is 0.512 and for Brand is 1.8 * 10^-13, it shows only Brand is a significant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference:\n",
    "    1) here the pvalue 0.5518 for the hypothesis 1 is greater than .05 hence we are accepting the null nypothesis \n",
    "        and conclude that board_ssc has an effect on salary\n",
    "    2) here the pvalue 0.8933 for the hypothesis 2 is greater than .05 hence we are accepting the null nypothesis \n",
    "        and conclude that Course_Degree has an effect on salary\n",
    "    3) here the pvalue 0.509 for the hypothesis 2 is greater than .05 hence we are accepting the null nypothesis \n",
    "        and conclude that interaction between Board_SSC:Course_Degree have affect on salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. chi-square test : one-way\n",
    "- to compare between observed and expected of the same group.that is only one single column in picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h0: there is no change in expected value and observed value\n",
    "#h1: there is a change in expected value and obsrved value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square_stat, p_value = stats.chisquare(observed_values, f_exp=expected_values)\n",
    "print('At 5 %s level of significance, the p-value is %1.7f' %('%', p_value))\n",
    "print('At 5 %s level of significance, the chi observed is %1.7f' %('%', chi_square_stat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chi-square : two-way\n",
    "-to compare between two different categorical groups. that is two different columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taking categorical features from dataframe: (remember:dont take value counts , just pass column nmae. cross tab does value_counts internally)\n",
    "\n",
    "\n",
    "df_ct=pd.crosstab(df['gender'],df['Churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_sq_Stat, p_value, deg_freedom, exp_freq = stats.chi2_contingency(churn_array)  ## churn_array=[all values of group1]+[all valuesof group2]+[all values of group3]  ##group in the sense row wise\n",
    "print('Chi-square statistic %3.5f P value %1.6f Degrees of freedom %d' %(chi_sq_Stat, p_value,deg_freedom))\n",
    "\n",
    "chi_sq_Stat, p_value, deg_freedom, exp_freq = stats.chi2_contingency([[2549,939],[2625,930]])  ##two square brackets are imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference:\n",
    "this example, p value is 0.486579 and > 0.05 so we accept the null hypothesis.\n",
    "So, we conclude that There is no difference in churn based on gender nature 'male' or 'female'\n",
    "(we check whether churn is dependent on gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference:\n",
    "# Here since the pvalue is 0.070563 which is greater than 0.05, we will not reject the null hypothesis that there is not \n",
    "# significant differenc in quality among the three shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting/picking desired datatype features/columns\n",
    "df.select_dtypes(include='object').columns\n",
    "cat_cols\n",
    "\n",
    "cat_nums=df.select_dtypes(include=['float64','int64']).columns\n",
    "cat_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.get_dummies(data=df_main,columns=['gender','region'],drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference from data\n",
    "boxplots and outlier identification (from describe)\n",
    "outlier removal\n",
    "null vaue imputation\n",
    "dummies value to categorical columns\n",
    "rename columns if reqd\n",
    "check the possibility of multicollinearity (heatmap and vif method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df1['status']= label_encoder.fit_transform(df1['status']) \n",
    "  \n",
    "df1['status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA-easy way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling as Profile\n",
    "Profile.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for problem with graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outlier&distribution together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_outlier=df.select_dtypes('object')\n",
    "cols_outlier=list(cols_outlier)\n",
    "\n",
    "for i in range (0,len(cols)):\n",
    "    if (df[cols[i]].dtypes=='int64' or df[cols[i]].dtypes=='float64') & (cols[i] not in cols_outlier):\n",
    "        print(\"variable    :    \",cols[i])\n",
    "        sns.distplot(df[cols[i]],hist=False)\n",
    "        plt.show()\n",
    "        sns.boxplot(y=df[cols[i]])\n",
    "        plt.show()\n",
    "        print('\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### outlier removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range (0,len(outlier_list)):\n",
    "    \n",
    "        q3=df[outlier_list[i]].quantile(0.75)\n",
    "        q1=df[outlier_list[i]].quantile(0.25)\n",
    "        iqr=q3-q1\n",
    "        df=df[  (df[outlier_list[i]]>=q1-1.5*iqr)  &   (df[outlier_list[i]]<=q3+1.5*iqr)   ]\n",
    "\n",
    "        \n",
    "##remember to reset the index after removal, as there is a chance of jumbled up index number which can cause problem while concat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking whether sample is as same as population\n",
    "-a.\tAre both train and test representative of the overall data? How would you ascertain this statistically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one sample t test:\n",
    "\n",
    "#we shall use one sample t test to check the hypothesis:\n",
    "#fi nd mean and std deviation of the target variable from dataframe\n",
    "print('Mean is %2.1f Sd is %2.1f' % (df['Churn'].mean(),np.std(df['Churn'],ddof = 1)))\n",
    "\n",
    "from scipy.stats             import ttest_1samp,ttest_ind, wilcoxon\n",
    "from statsmodels.stats.power import ttest_power\n",
    "\n",
    "\n",
    "#first check y_train with above mean\n",
    "t_statistic, p_value = ttest_1samp(y_train, df['churn'].mean())\n",
    "print(t_statistic, p_value)\n",
    "\n",
    "#second, check y_test with above mean\n",
    "t_statistic, p_value = ttest_1samp(y_test, df['churn'].mean())\n",
    "print(t_statistic, p_value)\n",
    "\n",
    "#explain about hypothesis. \n",
    "#ho: means of sample and population are same.\n",
    "#h1: means are different. we need h0 to be accpeted here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scaling/transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax=MinMaxScaler()\n",
    "df['new_column']=minmax.fit_transform(df[['total_bill']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scal=StandardScaler()\n",
    "X=pd.DataFrame(scal.fit_transform(x),columns=x.columns)  #we dont scale target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covariance & correlation:\n",
    "\n",
    "cov(x,y)= summation ( (x-x_mean)(y-y_mean)  ) / (n-1)\n",
    "\n",
    "cor(x,y)= cov(x,y) / x.std() * y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selecting categorical and numerical columns/features\n",
    "-selecting required datatypes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=df.select_dtypes(include=['int64','float64']).columns\n",
    "cat=df.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(list(num))   #numerical features\n",
    "print(list(cat))   #categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking whether sample is as same as population\n",
    "-a.\tAre both train and test representative of the overall data? How would you ascertain this statistically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1:\n",
    "#one sample t test:\n",
    "\n",
    "#we shall use one sample t test to check the hypothesis:\n",
    "print('Mean is %2.1f Sd is %2.1f' % (df['Churn'].mean(),np.std(df['Churn'],ddof = 1)))\n",
    "\n",
    "from scipy.stats             import ttest_1samp,ttest_ind, wilcoxon\n",
    "from statsmodels.stats.power import ttest_power\n",
    "\n",
    "t_statistic, p_value = ttest_1samp(df['Churn'], 0.1)\n",
    "print(t_statistic, p_value)\n",
    "\n",
    "#explain about hypothesis. \n",
    "#ho: means of sample and population are same.\n",
    "#h1: means are different. we need h0 to be accpeted here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 2:\n",
    "-check describe() of train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-when number of features are very less, there is a high chance of overfitting. to increase the features use polynomial feature engineering. it can improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axs=plt.subplots(2,2,figsize=(8,5) )\n",
    "sns.boxplot(df['Salary'],ax=axs[0][0])\n",
    "sns.kdeplot(df['Salary'],ax=axs[1][0])\n",
    "sns.boxplot(df['lsalary'],ax=axs[0][1])\n",
    "sns.kdeplot(df['lsalary'],ax=axs[1][1])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skeleton for modelling/regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1) Read Data\n",
    "2) Shape\n",
    "3)identify categorical and numerical features.\n",
    "        a) write inference with proportion of categorical data.\n",
    "3) Target variable imbalance (only in train set. apply model in test(test should be as is)\n",
    "        a)undersampling majority class\n",
    "        b)oversampling minority class\n",
    "        c)SMOTE-synthetic minority oversampling technique\n",
    "4) Null value imputation:\n",
    "        a)if missing value less than 10% use median/mean\n",
    "        b)more than 10%,bfill/ffill. (but it cant be used in cross sectional data where autocorrelation is zero)\n",
    "        c) more than 70%, make it into dummy variables(1-no miss, 0-miss data)\n",
    "        d) if target has missing values, drop it.\n",
    "5) 5 point summary (describe())\n",
    "        a) atleast write of two points\n",
    "6) plots/graphs\n",
    "        a)Important - bivariant analysis (target vs feature)\n",
    "        b) univariate analysis\n",
    "        c) correaltion- heatmap\n",
    "7) outlier detection and removal:\n",
    "        a) use iqr method ( if outlier are more, mean will be already compromised. hence, a score method wont be effective)\n",
    "        b)in regression, if target has outlier, dont remove it. instead transform the target variable to include outlier.\n",
    "8) categorical into dummies\n",
    "        a)dont convert target into dummies\n",
    "        b)if range of categorical columns are less(<=5), convert it into categorical(even if it is float/int). but only ater             understanding the data.\n",
    "        c)if range of categorical are high(>6), keep it as is.\n",
    "        d)always drop_first=True\n",
    "9) Train and test split\n",
    "        a)make data into standardized(z score) when algorithm has distance calcualtion in it.min_max method(normalization) is             only used in artificial neural network.\n",
    "        b)use 1 sample t test to check whether sample mean of test is same as train. ( sample vs population)\n",
    "        c)check 5 point summary of both test and train with inference\n",
    "10) Base model\n",
    "        a) statistical approach. Logistica regression is preferred mostly though\n",
    "        b)improve model by vif/feature selection/feature engineering (poly)\n",
    "11) machine learning techniques:\n",
    "        a) linear models can use feature selection (vif). ex: logistic and svm\n",
    "        b)improve model by \n",
    "                            a.1)hyperparamter tuning\n",
    "                            a.2)interaction/feature engineering/PCA\n",
    "        c)to get full effect of boarder line data, binning can be done and make it new variable.\n",
    "        d) regularization for overfitting/underfitting.\n",
    "12) Mapping all results and picking up best algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R square and Adjusted R square:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adj r square= 1 - (1-r2) *  (n-1)/ (n-k-1)\n",
    "\n",
    "r2= r square value\n",
    "n=number of observations\n",
    "k=no of independent variables-1\n",
    "\n",
    "when number of features increases, R ssquare will automatically increase irrespective of the significance the added feature has. so, in this case of adding insignificant feature,  k also increases. when k increases (n-k-1) decreases and whole value of (n-1)/ (n-k-1) incrases. hence, adjusted R square value decreases.\n",
    "\n",
    "when we add significant feature, R square value increases making (1-r2) *  (n-1)/ (n-k-1) value very less. hence, in this circumstance, adjusted r square value increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table for showing results: (after modelling)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table=pd.DataFrame({ 'R square value':[train_score_normal,test_score_normal,train_score1,test_score1,regressor_pf_train,regressor_pf_test,rfr_train,rfr_test],\n",
    "                           'RMSE value':[rmse_train_normal,rmse_test_normal,rmse_train1,rmse_test1,regressor_pf_rmse_train,regressor_pf_rmse_test,rfr_rmse_train,rfr_rmse_test]},\n",
    "                         index=['train_base model','test_base mode','train_after feature selection','test_after feature selection','train_polynomial','test_polynomial','train_random forrest','test_random forrest'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table\n",
    "result_table1=result_table.reset_index()\n",
    "result_table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,4))\n",
    "sns.barplot(x='index',y='R square value',data=result_table1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,4))\n",
    "sns.barplot(x='index',y='RMSE value',data=result_table1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# important features/significant features.\n",
    "-which variables are affecting the target the most and explain the relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr=RandomForestRegressor(random_state=1)\n",
    "rfr.fit(xp_df,yp_df)\n",
    "\n",
    "\n",
    "rfr.fit(X_nc,Y)\n",
    "cols=X_nc.columns\n",
    "rfr.feature_importances_\n",
    "fi=pd.DataFrame(rfr.feature_importances_,index=cols,columns=['importance'])\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to show full rows and columns:\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
