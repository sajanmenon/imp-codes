{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.standardization of data \n",
    "\n",
    "using zscore:\n",
    "from scipy.stats import zscore\n",
    "df_scaled=df2.apply(zsore)\n",
    "\n",
    "using Standardscaler:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "cols=df2.columns\n",
    "ss=StandardScaler()\n",
    "df_scaled=pd.DataFrame(ss.fit_transform(df2),columns=cols)  ##make cols[0:-1] if dataset has target given in it.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the clusterid and SSE in form of dataframe:\n",
    "from sklearn.cluster import KMeans\n",
    "cluster_range = range( 1, 15 )\n",
    "cluster_errors = []\n",
    "for num_clusters in cluster_range:\n",
    "  clusters = KMeans( num_clusters, n_init = 10 )\n",
    "  clusters.fit(df_scaled)   ## clsuters.fit_predict() will do the same job of clusters.fit()+clsuters.lables_\n",
    "  labels = clusters.labels_\n",
    "  centroids = clusters.cluster_centers_\n",
    "  cluster_errors.append( clusters.inertia_ )\n",
    "clusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\n",
    "clusters_df[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to draw elbow plot/scree plot with above results:\n",
    "# Elbow plot\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after selecting number of cluster we need from elbow plot, making kmeans model now:\n",
    "# Elbow plot\n",
    "\n",
    "kmeans = KMeans(n_clusters=6, n_init = 15, random_state=2345)\n",
    "kmeans.fit(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for list of centroids:\n",
    "centroids=kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_df = pd.DataFrame(centroids, columns = list(df_scaled) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a new dataframe only for labels and converting it into categorical variable\n",
    "df_labels = pd.DataFrame(kmeans.labels_ , columns = list(['labels']))\n",
    "\n",
    "df_labels['labels'] = df_labels['labels'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### important-reset_index\n",
    "Joining the label dataframe with the Wine data frame to create wine_df_labeled. Note: it could be appended to original dataframe\n",
    "\n",
    "\n",
    "wine_df_labeled = df2.join(df_labels)   (make sure, df2 index number is sequential. df2.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = (wine_df_labeled.groupby(['labels'] , axis=0)).head(1599)  # the groupby creates a groupeddataframe that needs \n",
    "# to be converted back to dataframe. I am using .head(30000) for that\n",
    "df_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each label(filter one by one),make different dataframes.\n",
    "then make a new dataframe with means of all features of different label_dataframes to get inferences from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0=pd.DataFrame(df_analysis_0.mean())  #dataframe of 0th cluster after filtering label column in the main dataset\n",
    "df_1=pd.DataFrame(df_analysis_1.mean())  ##dataframe of 1st cluster after filtering label column in the main dataset\n",
    "df_2=pd.DataFrame(df_analysis_2.mean())  ##dataframe of 2nd cluster after filtering label column in the main dataset\n",
    "df_3=pd.DataFrame(df_analysis_3.mean())  ##dataframe of 3rd cluster after filtering label column in the main dataset\n",
    "df_final=pd.concat([df_0,df_1,df_2,df_3],axis=1)\n",
    "df_final.columns=[0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting 3d plot:\n",
    "#3D Scatter Plot of the Clusters formed\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=20, azim=120)\n",
    "kmeans.fit(X_scaled)\n",
    "labels = kmeans.labels_\n",
    "ax.scatter(X_scaled.iloc[:, 0], X_scaled.iloc[:,2 ], X_scaled.iloc[:, 10],c=labels.astype(np.float), edgecolor='k')\n",
    "## change X_scaled into pd.DataFrame(X_scaled) if error pops up\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Alcohol')\n",
    "ax.set_ylabel('Falvanoids')\n",
    "ax.set_zlabel('Color_intensity')\n",
    "ax.set_title('3D plot of KMeans Clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting 2d scatter plots\n",
    "\n",
    "f , ax = plt.subplots ( figsize = ( 10 , 8 ) )\n",
    "sns.scatterplot(x='Al',y='Ca',data=x,hue='label',palette='rainbow')   #'label' is the column name where labels are stored in main dataframe.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Hierarchial Clustering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting dendrogram:\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "plt.figure(figsize=[10,10])\n",
    "merg = linkage(df1_std, method='ward')    #change df name here\n",
    "dendrogram(merg, leaf_rotation=90)\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Euclidean Distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do cophenetic distance calculation for getting best linkage method: then do clustering with this linkage method\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage,dendrogram\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import cophenet, dendrogram, linkage\n",
    "    \n",
    "Z = linkage(X_scaled, method='complete')\n",
    "c, coph_dists = cophenet(Z , pdist(X_scaled))\n",
    "c\n",
    "\n",
    "#repeat this 4 times by changing the method and compare the value and the select the method having value close to 1.\n",
    "#Use Cophenet index to find best linkage method\n",
    "\n",
    "#or\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage,dendrogram\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import cophenet, dendrogram, linkage\n",
    "methods=['average','complete','ward','single','centroid']\n",
    "for i in methods:\n",
    "    Z = linkage(df_scaled, method=i)\n",
    "    c, coph_dists = cophenet(Z , pdist(df_scaled))\n",
    "    print('cophenetic distance by the method ',i,'is',c)\n",
    "\n",
    "\n",
    "\n",
    "# cophenet index is a measure of the correlation between the distance of points in feature space and distance on dendrogram\n",
    "# closer it is to 1, the better is the clustering,it represents the tightness of the clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hierarchial model\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "hie_clus = AgglomerativeClustering(n_clusters=2,affinity='euclidean', linkage='ward')\n",
    "cluster2 = hie_clus.fit_predict(df1_std)  ##hie_clus.fit(df1_std)---> and then hie_clus.label_ also works same. both gives predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just follow the steps written in kmeans. after you label it, make a new dataframe of this lables and join this to main dataset.\n",
    "assume we have 4 classes(labels). now from main dataset, filter each label and make new dataframe.\n",
    "now make one more datframe using means of all features in 4 dataframes so that we can compare and get inference from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plotting 3d graph\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=10, azim=120) # elev and azim can change view \n",
    "hier.fit(x_std)\n",
    "labels = hier.labels_\n",
    "ax.scatter(x_std.iloc[:, 5],x_std.iloc[:, 6], x_std.iloc[:, 0],c=labels.astype(np.float), edgecolor='red')\n",
    "#make pd.DataFrame(x_std).iloc[:,5]\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Total_phenols')\n",
    "ax.set_ylabel('Falvanoids')\n",
    "ax.set_zlabel('Alchol')\n",
    "ax.set_title('3D plot of Hierarchical Clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.DBSCAN (density based spatial clustering of applications with noise.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize DBSCAN\n",
    "db=DBSCAN(eps=0.2,min_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit on the data\n",
    "db.fit(df[['Xcircle_X1','Xcircle_X2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### checking how many clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(db.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DB_Cluster']=db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Xcircle_X1',y='Xcircle_X2',hue='DB_Cluster',data=df,palette='spring')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Customer Segmentation (market BAsket Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for changing into date_time format\n",
    "df['InvoiceDate']=pd.to_datetime(df['InvoiceDate'],infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating RFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.Monetary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount=pd.DataFrame(order_wise.Quantity*order_wise.UnitPrice,columns=['Amount']) #making a new feature\n",
    "order_wise=pd.concat(objs=[order_wise,amount],axis=1,ignore_index=False)#joining the feature into main dataframe\n",
    "monetary=order_wise.groupby('CustomerID').Amount.sum() # grouping by id to get unique customer details\n",
    "monetary=monetary.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = order_wise[['CustomerID','InvoiceNo']]\n",
    "k=frequency.groupby('CustomerID').InvoiceNo.count()\n",
    "k=pd.DataFrame(k)\n",
    "k=k.reset_index()\n",
    "k.columns=['CustomerID','Frequency']\n",
    "k.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency=order_wise[['CustomerID','InvoiceDate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#to get the how recent, current date-invoice date\n",
    "#whenever we do date calculation, there are two approaches.days with zero and days without zero.\n",
    "#here we are using days without zero. this means even if a person buys a product on the date of maximum (very recent purchase, it shuldn't show zero. it should show 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum=max(recency.InvoiceDate)  #getting latest date from invoicedate column\n",
    "maximum=maximum+pd.DateOffset(days=1)  #day without zero. \n",
    "recency['diff']=maximum-recency.InvoiceDate\n",
    "df=pd.DataFrame(recency.groupby('CustomerID').diff.min())\n",
    "df.head()\n",
    "\n",
    "df=df.reset_index()\n",
    "df.columns=['CustomerID','Recency']\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### merging & clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will merge all these 3 features into a new dataframe and then do all the model building process on it.\n",
    "#go through all the model creation points.-skeleton on this\n",
    "RFM=k.merge(monetary,on='CustomerID')\n",
    "RFM=RFM.merge(df,on='CustomerID')\n",
    "RFM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remaining process:\n",
    "\n",
    "RFM_norm1=RFM.drop('CustomerID',axis=1)\n",
    "RFM_norm1.Recency=RFM_norm1.Recency.dt.days\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standard_scaler=StandardScaler()\n",
    "data=standard_scaler.fit_transform(RFM_norm1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after scree plot and doing kmeans , we got optimum number of clusters as 4.\n",
    "model_clu5=KMeans(n_clusters=4,max_iter=50)\n",
    "model_clu5.fit(RFM_norm1)\n",
    "\n",
    "\n",
    "RFM.index=pd.RangeIndex(len(RFM.index))\n",
    "RFM_km=pd.concat([RFM,pd.Series(model_clu5.labels_)],axis=1)\n",
    "RFM_km.columns=['CustomerID','Frequency','Amount','Recency','ClusterID']\n",
    "\n",
    "RFM_km.Recency=RFM_km.Recency.dt.days\n",
    "km_clusters_amount=pd.DataFrame(RFM_km.groupby(['ClusterID']).Amount.mean())\n",
    "km_clusters_frequency=pd.DataFrame(RFM_km.groupby(['ClusterID']).Frequency.mean())\n",
    "km_clusters_recency=pd.DataFrame(RFM_km.groupby(['ClusterID']).Recency.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([pd.Series([0,1,2,3]),km_clusters_amount,km_clusters_frequency,km_clusters_recency],axis=1)\n",
    "df.columns=['ClusterID','Amount_mean','Frequency_mean','Recency_mean']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### plotting graphs for understanding cluster characterstics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#monetary\n",
    "sns.barplot(x=df.ClusterID,y=df.Amount_mean)\n",
    "plt.show()\n",
    "#frequency\n",
    "sns.barplot(x=df.ClusterID,y=df.Frequency_mean)\n",
    "plt.show()\n",
    "#recency\n",
    "sns.barplot(x=df.ClusterID,y=df.Recency_mean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### above method or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis_0=df_analysis[df_analysis['predicted_labels']==0]\n",
    "df_analysis_1=df_analysis[df_analysis['predicted_labels']==1]\n",
    "df_analysis_2=df_analysis[df_analysis['predicted_labels']==2]\n",
    "df_analysis_3=df_analysis[df_analysis['predicted_labels']==3]\n",
    "\n",
    "df_0=pd.DataFrame(df_analysis_0.mean())\n",
    "df_1=pd.DataFrame(df_analysis_1.mean())\n",
    "df_2=pd.DataFrame(df_analysis_2.mean())\n",
    "df_3=pd.DataFrame(df_analysis_3.mean())\n",
    "\n",
    "\n",
    "df_final=pd.concat([df_0,df_1,df_2,df_3],axis=1)\n",
    "df_final.columns=[0,1,2,3]\n",
    "df_final.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle component Analysis (PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step1:standardization (for centering the data)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss=StandardScaler()\n",
    "x_std=ss.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step2: pairplot and covariance matrix\n",
    "\n",
    "sns.pairplot(x)\n",
    "x_cov=np.cov(x_std,rowvar=False)   ##rowvar is very important. this gives featues*features covariation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step3:obtaining eigen vecors and eigen values\n",
    "\n",
    "#Calculate eigenvalues and eigenvectors \n",
    "eigenvalues, eigenvectors, = np.linalg.eig(cov_matrix)\n",
    "print('Eigenvectors of Cov(X): \\n', eigenvectors)\n",
    "print('\\n\\n\\n')\n",
    "print('eigenvalues of cov(x) are :   \\n',eigenvalues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 (continued): Sort eigenvalues in descending order\n",
    "\n",
    "# Make a set of (eigenvalue, eigenvector) pairs\n",
    "eig_pairs = [(eigenvalues[index], eigenvectors[:,index]) for index in range(len(eigenvalues))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) pairs from highest to lowest with respect to eigenvalue\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "\n",
    "# Extract the descending ordered eigenvalues and eigenvectors\n",
    "eigvalues_sort = [eig_pairs[index][0] for index in range(len(eigenvalues))]\n",
    "eigvectors_sort = [eig_pairs[index][1] for index in range(len(eigenvalues))]\n",
    "\n",
    "# Let's confirm our sorting worked, print out eigenvalues\n",
    "print('Eigenvalues in descending order: \\n%s' %eigvalues_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eigenvalues)\n",
    "var_explained = [(i / tot) for i in sorted(eigenvalues, reverse=True)]  # an array of variance explained by each \n",
    "# eigen vector... there will be 8 entries as there are 8 eigen vectors)  #make i*100/tot to get percentage\n",
    "cum_var_exp = np.cumsum(var_explained)  # an array of cumulative variance. There will be 8 entries with 8 th entry \n",
    "# cumulative reaching almost 100%\n",
    "\n",
    "\n",
    "plt.bar(range(1,9), var_explained, alpha=0.5, align='center', label='individual explained variance')\n",
    "plt.step(range(1,9),cum_var_exp, where= 'mid', label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Project data onto lesser dimensions\n",
    "\n",
    "#keeping only first 6 principle components out of 8.\n",
    "P_reduce = np.array(eigvectors_sort[0:6]).transpose()\n",
    "\n",
    "Proj_data_2D = np.dot(X_std,P_reduce)   # we need to find u (m*k) that is observations in new dimensions.for that V-transpose \n",
    "                                        (transpose of eigen vector matrix ) * x_std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 6:making model in this principle components\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(Proj_data_2D, y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Measurement (euclidean,chebyshev,minkowski,jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial.distance as dist\n",
    "\n",
    "dist.euclidean ( data [ \"Ca\"] , data [ \"Fe\" ] ) \n",
    "dist.chebyshev ( data [ \"Ca\"] , data [ \"Fe\" ] )\n",
    "dist.minkowski ( data [ \"Ca\"] , data [ \"Fe\" ] )\n",
    "dist.jaccard ( data [ \"Ca\" ] , data [ \"Fe\" ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples,silhouette_score\n",
    "\n",
    "kmeans=KMeans(n_clusters=2)\n",
    "\n",
    "X=df3\n",
    "\n",
    "model = kmeans.fit(X=df3)\n",
    "\n",
    "y=model.labels_\n",
    "\n",
    "\n",
    "\n",
    "print(silhouette_score(X,y))\n",
    "\n",
    "score = []\n",
    "for n_clusters in range(2,10):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    score.append(silhouette_score(X, labels, metric='euclidean'))\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(score)\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"Silouette Score\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.title(\"Silouette for K-means\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
