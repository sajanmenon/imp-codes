{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.using statsmodels (statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['TenYearCHD']\n",
    "x=df.drop('TenYearCHD',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X=sm.add_constant(x)\n",
    "model=sm.Logit(y,X)\n",
    "result=model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for checking multicollinearity##      vif method\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "pd.DataFrame({'vif': vif[0:]}, index=X.columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# backward elimination for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(X.columns)\n",
    "p= []\n",
    "while (len(cols)>1):\n",
    "    \n",
    "    X = X[cols]\n",
    "                                ##if constant is added already, remove this.\n",
    "    model = sm.Logit(y,X).fit().pvalues\n",
    "    p = pd.Series(model.values[1:],index =X.columns[1:])      \n",
    "    pmax = max(p)\n",
    "    pid=p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(pid)\n",
    "        print('variable removed:',pid,pmax)\n",
    "    else:\n",
    "        break\n",
    "cols\n",
    "\n",
    "\n",
    "##already the remianing columns are removed from the dataframe. run sm.logit directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=sm.Logit(y,X)\n",
    "result=model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parameters or coeffecients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.predict() ## for predicting the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for taking exponential value for assumption number three:\n",
    "np.exp(result.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RevolvingUtilizationOfUnsecuredLines    4.204060\n",
    "age                                     0.945293\n",
    "DebtRatio                               0.725279\n",
    "MonthlyIncome                           0.999896\n",
    "NumberOfOpenCreditLinesAndLoans         1.003080\n",
    "NumberRealEstateLoansOrLines            1.167603\n",
    "NumberOfDependents                      1.032890"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assumptions from logistic regression model (coeffecient summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference from model:  (in linear regression connection is straigh forward unlike logistic regression)\n",
    "#checking signs\n",
    "1) when age increases chance of delinquency decreases\n",
    "2) when debtratio increases chance of delinquency increases.\n",
    "\n",
    "#checking log(odds)\n",
    "1) as age increases by 1 year,log(odds(deliquency)) decreases by .02\n",
    "2) as number of dependents increases by 1,log(odds(deliquency)) increases by .08.\n",
    "\n",
    "#checking exponential \n",
    "1) as age increases by 1 year, odds(delinquency)decreases by 0.02 (2%) (1-.02)\n",
    "2) as number of dependents increases by 1, odds(delinquency) increases by 0.08 (8%)\n",
    "3)in case of RevolvingUtilizationOfUnsecuredLines,DebtRatio etc, it shows more than 1. in this cases we need to modify the features before modelling so tnat we get meaniful ratios here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confusion matrix for statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob=result.predict(X[cols])\n",
    "prob=pd.DataFrame(prob,columns=['prob'])\n",
    "prob['y_est']=prob['prob'].apply(lambda x:1 if x>=0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "confusion_matrix(y,prob['y_est'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y,prob['y_est'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y,prob['y_est']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.using sklearn(machine learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression(solver='liblinear',fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for coeffecients\n",
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find accuracy from confusion matrix: we need to filter our model's probability by a threshold and compare with actual y_test value.\n",
    "\n",
    "y_prob=logreg.predict_proba(x_test)  # gives model's prediction. take logreg.predict_proba(x_test)[:,1], for probability of y being 1\n",
    "y_pred=logreg.predict(x_test)        # filters the y_prob by threshold 0.5 defualt. below code for changing 0.5 to other values.\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "confusion_matrix(y_test,y_pred)\n",
    "accuracy_score(y_test,y_pred)\n",
    "print(classification_report(y,pro\n",
    "                            b['y_est']))\n",
    "\n",
    "print('\\n','confusion matrix - Test : ','\\n',confusion_matrix(y_test,y_pred) )\n",
    "print('overall accuracy - Test : ',accuracy_score(y_test,y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when you need to manually put threshold and calculate y_pred:\n",
    "yprob=pd.DataFrame(y_prob[:,1],columns=['yprob'])\n",
    "yprob['ypred']=yprob['yprob'].apply(lambda x: 1 if x>0.4 else 0)   ##threshold 0.4 here.\n",
    "confusion_matrix(y_test,yprob['ypred'])\n",
    "accuracy_score(y_test,yprob['ypred'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##to plot the curve to understand the concept of threshold(0.5),tpr,fpr :\n",
    "\n",
    "y_prob_test=logreg.predict_proba(x_test)[:,1]   #slicing done to select only probability of (target being 1)\n",
    "prob_plot=pd.DataFrame([y_prob_test,y_test]).T\n",
    "\n",
    "prob_plot1=prob_plot[(prob_plot[1]==0)]  # filtering to get probability of target when actual value is 0 and 1\n",
    "prob_plot2=prob_plot[(prob_plot[1]==1)]\n",
    "\n",
    "sns.distplot(prob_plot1[0])\n",
    "sns.distplot(prob_plot2[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression(solver='liblinear',fit_intercept=True)\n",
    "logreg.fit(x_train,y_train)\n",
    "y_prob_train=logreg.predict_proba(x_train)[:,1]\n",
    "y_pred_train=logreg.predict(x_train)\n",
    "y_prob=logreg.predict_proba(x_test)[:,1]\n",
    "y_pred=logreg.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "roc_auc_score(y_train,y_prob_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "\n",
    "print('AUC of train  : ',roc_auc_score(y_train,y_prob_train))\n",
    "fpr,tpr,thresholds=roc_curve(y_train,y_prob_train)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot(fpr,fpr,'r-') ##for the diagnal line\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n",
    "plt.title('ROC of train data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ROC curve- same as above. micro level details added here.\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "\n",
    "print('AUC of train  : ',roc_auc_score(y_test,y_test_proba))\n",
    "                                    ##origina value vs our prediction value\n",
    "                                    #make sure y_test_proba=logreg.predict_proba(x_test)[:,1]. slicing for taking probability of y=1\n",
    "\n",
    "fpr,tpr,thresholds=roc_curve(y_test,y_test_proba)  #make sure y_test/y_train is not dataframe. it should be just array. \n",
    "                                                    #just drop y feature in the dataset.\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot(fpr,fpr,'r-') ##for the diagnal line\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n",
    "plt.title('ROC of train data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decission tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('TenYearCHD', axis=1)\n",
    "y = df['TenYearCHD']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3, min_samples_leaf=10 )\n",
    "dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "\n",
    "features = X.columns\n",
    "dot_data = export_graphviz(dt, out_file=None, feature_names=features)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = dt.predict(X_train)   ##takes 0.5 as threshold default. us manual program if threshold to be changed from 0.5\n",
    "y_pred_test = dt.predict(X_test)\n",
    "y_prob_test = dt.predict_proba(X_test)[:,1]\n",
    "y_prob_train = dt.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "print('Accuracy of Decision Tree-Train: ', accuracy_score(y_pred_train, y_train))\n",
    "print('Accuracy of Decision Tree-Test: ', accuracy_score(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc-roc:\n",
    "print('AUC of train  : ',roc_auc_score(y_train,y_prob_train[:,1]))\n",
    "\n",
    "fpr,tpr,thresholds=roc_curve(y_train,y_prob_train[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot(fpr,fpr,'r-') ##for the diagnal line\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n",
    "plt.title('ROC of train data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gini of each leaf:\n",
    "gini_1stleaf=1-(probability of yes)**2-(probability of no)**2\n",
    "gini_2ndleaf=1-(probability of yes)**2-(probability of no)**2\n",
    "\n",
    "gini of the column having above two leaf=weighted average of above two.\n",
    "{1stleaf('0's+'1's)/both_leaf's('0's+'1's)}*gini_1stleaf        +      {2ndleaf('0's+'1's)/both_leaf's('0's+'1's)}*gini_2ndleaf    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameter tuning in Decission Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "params = {'max_depth' : [2,3,4,5,6,7,8],\n",
    "        'min_samples_split': [2,3,4,5,6,7,8,9,10],\n",
    "        'min_samples_leaf': [1,2,3,4,5,6,7,8,9,10]}\n",
    "\n",
    "gsearch = GridSearchCV(dt, param_grid=params, cv=2)\n",
    "\n",
    "gsearch.fit(X,y)\n",
    "\n",
    "gsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use these parameter to build the decission tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(**gsearch.best_params_)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = dt.predict(X_train)\n",
    "y_prob_train = dt.predict_proba(X_train)[:,1]\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "y_prob = dt.predict_proba(X_test)[:,1]\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "print('Accuracy of Decision Tree-Train: ', accuracy_score(y_pred_train, y_train))\n",
    "print('Accuracy of Decision Tree-Test: ', accuracy_score(y_pred, y_test))\n",
    "print('AUC of Decision Tree-Train: ', roc_auc_score(y_train, y_prob_train))\n",
    "print('AUC of Decision Tree-Test: ', roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "params = {'max_depth' : sp_randint(2,10),\n",
    "        'min_samples_split': sp_randint(2,50),\n",
    "        'min_samples_leaf': sp_randint(1,20),\n",
    "         'criterion':['gini', 'entropy']}\n",
    "rand_search = RandomizedSearchCV(dt, param_distributions=params, cv=3, random_state=1)\n",
    "rand_search.fit(X, y)\n",
    "print(rand_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a decision tree with these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(**rand_search.best_params_)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = dt.predict(X_train)\n",
    "y_prob_train = dt.predict_proba(X_train)[:,1]\n",
    "y_pred = dt.predict(X_test)\n",
    "y_prob = dt.predict_proba(X_test)[:,1]\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "print('Accuracy of Decision Tree-Train: ', accuracy_score(y_pred_train, y_train))\n",
    "print('Accuracy of Decision Tree-Test: ', accuracy_score(y_pred, y_test))\n",
    "print('AUC of Decision Tree-Train: ', roc_auc_score(y_train, y_prob_train))\n",
    "print('AUC of Decision Tree-Test: ', roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier(random_state=1)\n",
    "\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_pred_train=rfc.predict(X_train)\n",
    "y_prob_train=rfc.predict_proba(X_train)[:,1]\n",
    "y_pred_test=rfc.predict(X_test)\n",
    "y_prob_test=rfc.predict_proba(X_test)[:,1]\n",
    "\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve\n",
    "print('accuracy of random forest-train  :  ', accuracy_score(y_pred_train,y_train))\n",
    "print('accuracy of random forest-test  :  ', accuracy_score(y_pred_test,y_test))\n",
    "print('AUC of random forest-train  :  ', roc_auc_score(y_train,y_prob_train))\n",
    "print('AUC of random forest-test  :  ', roc_auc_score(y_test,y_prob_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameter tuning for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "rfc=RandomForestClassifier(random_state=1)\n",
    "\n",
    "params={'n_estimators':sp_randint(5,25),\n",
    "    'criterion':['gini','entropy'],\n",
    "    'max_depth':sp_randint(2,10),\n",
    "    'min_samples_split':sp_randint(2,20),\n",
    "    'min_samples_leaf':sp_randint(1,20),\n",
    "    'max_features':sp_randint(2,15)}\n",
    "\n",
    "rand_search_rfc=RandomizedSearchCV(rfc,param_distributions=params,\n",
    "                               cv=3,random_state=1)\n",
    "\n",
    "\n",
    "rand_search_rfc.fit(X,y)\n",
    "print(rand_search_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now make random forest using above optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier(**rand_search_rfc.best_params_)\n",
    "\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_pred_train=rfc.predict(X_train)\n",
    "y_prob_train=rfc.predict_proba(X_train)[:,1]\n",
    "y_pred_test=rfc.predict(X_test)\n",
    "y_prob_test=rfc.predict_proba(X_test)[:,1]\n",
    "\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve\n",
    "print('accuracy of random forest-train  :  ', accuracy_score(y_pred_train,y_train))\n",
    "print('accuracy of random forest-test  :  ', accuracy_score(y_pred_test,y_test))\n",
    "print('AUC of random forest-train  :  ', roc_auc_score(y_train,y_prob_train))\n",
    "print('AUC of random forest-test  :  ', roc_auc_score(y_test,y_prob_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,thresholds=roc_curve(y_test,y_prob_train)\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot(fpr,fpr,'r-')\n",
    "plt.xlabel('Fpr')\n",
    "plt.ylabel('Tpr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-NN classifier\n",
    "always standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  here transformation of all featuers (only) are done. not target.\n",
    "#  transformation is based on zscore.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "Xs = ss.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "X_trains = ss.fit_transform(X_train)\n",
    "X_tests = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_trains, y_train)\n",
    "\n",
    "y_pred_train = knn.predict(X_trains)\n",
    "y_prob_train = knn.predict_proba(X_trains)[:,1]\n",
    "\n",
    "y_pred_test = knn.predict(X_tests)\n",
    "y_prob_test = knn.predict_proba(X_tests)[:,1]\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "print('Accuracy of kNN-Train: ', accuracy_score(y_pred_train, y_train))\n",
    "print('Accuracy of kNN-Test: ', accuracy_score(y_pred_test, y_test))\n",
    "print('AUC of kNN-Train: ', roc_auc_score(y_train, y_prob_train))\n",
    "print('AUC of kNN-Test: ', roc_auc_score(y_test, y_prob_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter tuning for k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "params = {'n_neighbors': sp_randint(1,20),\n",
    "        'p': sp_randint(1,5)}\n",
    "\n",
    "rand_search_knn = RandomizedSearchCV(knn, param_distributions=params,\n",
    "                                 cv=3, random_state=1)\n",
    "rand_search_knn.fit(Xs, y)\n",
    "print(rand_search_knn.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now make a new k-NN model with all above optimum parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(**rand_search_knn.best_params_)\n",
    "\n",
    "knn.fit(X_trains, y_train)\n",
    "\n",
    "y_pred_train = knn.predict(X_trains)\n",
    "y_prob_train = knn.predict_proba(X_trains)[:,1]\n",
    "\n",
    "y_pred_test = knn.predict(X_tests)\n",
    "y_prob_test = knn.predict_proba(X_tests)[:,1]\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "print('Accuracy of kNN-Train: ', accuracy_score(y_pred_train, y_train))\n",
    "print('Accuracy of kNN-Test: ', accuracy_score(y_pred_test, y_test))\n",
    "print('AUC of kNN-Train: ', roc_auc_score(y_train, y_prob_train))\n",
    "print('AUC of kNN-Test: ', roc_auc_score(y_test, y_prob_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding optimum number of neighbour using MSE(miss classification error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating odd list of K for KNN\n",
    "myList = list(range(1,20))\n",
    "\n",
    "# subsetting just the odd ones\n",
    "neighbors = list(filter(lambda x: x % 2 != 0, myList))\n",
    "\n",
    "\n",
    "# empty list that will hold accuracy scores\n",
    "ac_scores = []\n",
    "\n",
    "# perform accuracy metrics for values from 1,3,5....19\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    # predict the response\n",
    "    y_pred = knn.predict(X_test)\n",
    "    # evaluate accuracy\n",
    "    scores = accuracy_score(y_test, y_pred)\n",
    "    ac_scores.append(scores)\n",
    "\n",
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in ac_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d\" % optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plotting missclassification error vs number_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## miss classification eror(MSE)  is 1-accuracy score. find it for each iteartion and plot graph.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plot misclassification error vs k\n",
    "plt.plot(neighbors, MSE)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression(solver='liblinear')\n",
    "rfc=RandomForestClassifier(**rand_search_rfc.best_params_)\n",
    "knn=KNeighborsClassifier(**rand_search_knn.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=VotingClassifier(estimators=[('lr',lr),('rfc',rfc),('knn',knn)])\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=VotingClassifier(estimators=[('lr',lr),('rfc',rfc),('knn',knn)],voting='soft',weights=[2,3,1])  #for lr weigtage is 2/5-40%. for rf, 3/6=33%, for knn 1/6%\n",
    "                                                                                                    #'remove weights=' for equal weightage.\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train=clf.predict(X_train)\n",
    "y_prob_train=clf.predict_proba(X_train)[:,1]\n",
    "\n",
    "#y_pred=clf.predict(X_test)\n",
    "y_prob=clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy of stacked algos-train  :  ', accuracy_score(y_pred_train,y_train))\n",
    "print('accuracy of stacked algos-test  :  ', accuracy_score(y_pred,y_test))\n",
    "print('AUC of stacked algos-train  :  ', roc_auc_score(y_train,y_prob_train))\n",
    "print('AUC of stacked algos-test  :  ', roc_auc_score(y_test,y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient boosting machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# light gradient boosting machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "lgbc=lgb.LGBMClassifier()\n",
    "X=\n",
    "y=\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbc.fit(X_train,y_train)\n",
    "y_pred=lgbc.predict(X_test)\n",
    "y_prob=lgbc.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve\n",
    "print('Accuracy on test set  ', accuracy_score(y_test,y_pred))\n",
    "print('AUC on test set  ', roc_auc_score(y_test,y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=X.columns\n",
    "lgbc.feature_importances_\n",
    "fi=pd.DataFrame(lgbc.feature_importances_,index=cols,columns=['importance'])\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=X.columns\n",
    "lgbc.feature_importances_\n",
    "fi=pd.DataFrame(lgbc.feature_importances_,index=cols,columns=['importance'])\n",
    "fi=fi.sort_values('importance',ascending=False)\n",
    "fi.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparamter tuning for lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "lgbc=lgb.LGBMClassifier(random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params={'n_estimators':sp_randint(5,250),\n",
    "        'max_depth':sp_randint(2,20),\n",
    "        'min_samples_split':sp_randint(2,20),\n",
    "        'min_samples_leaf':sp_randint(1,20),\n",
    "        'num_leaves':sp_randint(5,50)}\n",
    "\n",
    "rand_search_lgbc=RandomizedSearchCV(lgbc,param_distributions=params,\n",
    "                               cv=3,random_state=1)\n",
    "\n",
    "\n",
    "rand_search_lgbc.fit(X,y)\n",
    "print(rand_search_lgbc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbc=lgb.LGBMClassifier(**rand_search_lgbc.best_params_,random_state=1)\n",
    "lgbc.fit(X_train,y_train)\n",
    "    \n",
    "y_pred_train=lgbc.predict(X_train)\n",
    "y_prob_train=lgbc.predict_proba(X_train)[:,1]\n",
    "    \n",
    "y_pred=lgbc.predict(X_test)\n",
    "y_prob=lgbc.predict_proba(X_test)[:,1]\n",
    "    \n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve\n",
    "    \n",
    "print('Accuracy of LGBM-Train ',accuracy_score(y_pred_train,y_train) )\n",
    "print('Accuracy of LGBM-test',accuracy_score(y_pred,y_test) )\n",
    "    \n",
    "    \n",
    "print('AUC ofLGBM-train ',roc_auc_score(y_train,y_prob_train) )\n",
    "print('AUC of LGBM-Test ',roc_auc_score(y_test,y_prob) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,thresholds=roc_curve(y_test,y_prob)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot(fpr,fpr,'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc=SVC(gamma='auto',probability=True)\n",
    "\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_pred_train=svc.predict(X_train)\n",
    "y_prob_train=svc.predict_proba(X_train)[:,1]\n",
    "y_pred=svc.predict(X_test)\n",
    "y_prob=svc.predict_proba(X_test)[:,1]\n",
    "\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve\n",
    "print('accuracy of svc-train  :  ', accuracy_score(y_pred_train,y_train))\n",
    "print('accuracy of svc-test  :  ', accuracy_score(y_pred,y_test))\n",
    "print('auc of svc-train  :  ', roc_auc_score(y_train,y_prob_train))\n",
    "print('auc of svc-test  :  ', roc_auc_score(y_test,y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameter tuning for svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "svc=SVC(probability=True)\n",
    "\n",
    "params={'C': np.logspace(-4,4,10000),\n",
    "       'gamma':np.logspace(-4,4,10000)}\n",
    "\n",
    "rand_search_svc=RandomizedSearchCV(svc,param_distributions=params,\n",
    "                               cv=3,random_state=1)\n",
    "\n",
    "rand_search_svc.fit(X,y)\n",
    "print(rand_search_svc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc=SVC(**rand_search_svc.best_params_,probability=True,random_state=1)\n",
    "svc.fit(X_train,y_train)\n",
    "    \n",
    "y_pred_train=svc.predict(X_train)\n",
    "y_prob_train=svc.predict_proba(X_train)[:,1]\n",
    "    \n",
    "y_pred=svc.predict(X_test)\n",
    "y_prob=svc.predict_proba(X_test)[:,1]\n",
    "    \n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve\n",
    "    \n",
    "print('Accuracy of svc-Train ',accuracy_score(y_pred_train,y_train) )\n",
    "print('Accuracy of svc-test',accuracy_score(y_pred,y_test) )\n",
    "    \n",
    "    \n",
    "print('AUC of svc-train ',roc_auc_score(y_train,y_prob_train) )\n",
    "print('AUC of svc-Test ',roc_auc_score(y_test,y_prob) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class']=df['Class'].map({'ham':0,'spam':1})   ##making two classes of target into 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=df['sms']\n",
    "y=df['Class']\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(stop_words='english')   ##stop_words=englsh, will remove stop words such asconnecting words like if,else,so etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.fit(X_train)\n",
    "X_train_tr=vect.transform(X_train)\n",
    "X_test_tr=vect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "bnb=BernoulliNB()\n",
    "\n",
    "bnb.fit(X_train_tr,y_train)\n",
    "y_pred=bnb.predict(X_test_tr)\n",
    "y_pred_proba=bnb.predict_proba(X_test_tr)[:,1]\n",
    "\n",
    "print('Accuracy score on Test  : ', accuracy_score(y_test,y_pred))\n",
    "print('AUC on Test  : ', roc_auc_score(y_test,y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis=pd.DataFrame({'logistic regression':[lr_accuracy,lr_roc],'random forrest':[rf_accuracy,rf_roc],'K-NN':[knn_accuracy,knn_roc]},index=['accuracy_value','roc-auc_value'])\n",
    "\n",
    "df_analysis\n",
    "df_analysis.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "df['status']=le.fit_transform(df['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Imbalance Data(undersampling,oversampling,SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "say we got [0]-.998% and [1]-.001%, we have imbalance Data here.\n",
    "#we will do treatment of imbalance data in train data only. let model learn from it. and apply this model on test data as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.Undersampling Majority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train=pd.concat([X_train,y_train],axis=1)\n",
    "#splitting 0,1 now\n",
    "Xy_train0=Xy_train.loc[Xy_train['Class']==0]\n",
    "Xy_train1=Xy_train.loc[Xy_train['Class']==1]\n",
    "#we will undersample 0 now\n",
    "\n",
    "len1=len(Xy_train1)\n",
    "len0=len(Xy_train0)\n",
    "\n",
    "Xy_train0_us=Xy_train0.sample(len1)\n",
    "Xy_train_us=pd.concat([Xy_train0_us,Xy_train1],axis=0)\n",
    "\n",
    "y_train_us=Xy_train_us['Class']\n",
    "X_train_us=Xy_train_us.drop('Class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train_us)  ##we made zeroes to match with 1. hence 357+357=714\n",
    "print(len(y_train),len(y_test))\n",
    "print(len(y_train[y_train==1]),len(y_test[y_test==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we undersampled the majority class (0) to make it equal to minority class(1).\n",
    "this exercise was done only for train set.test set was left untouched.\n",
    "build the model in rtain set and try in test set. check whether there is improvement from the results of model of imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier(**rand_search_rfc.best_params_)\n",
    "\n",
    "model_compare(rfc,X_train_us,X_test,y_train_us,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.Oversampling of Minority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train=pd.concat([X_train,y_train],axis=1)  ##x and y data together\n",
    "#splitting two datasets of 0,1 now\n",
    "Xy_train0=Xy_train.loc[Xy_train['Class']==0]\n",
    "Xy_train1=Xy_train.loc[Xy_train['Class']==1]\n",
    "\n",
    "#we will oversample 1 to make it as same as 0.\n",
    "len1=len(Xy_train1)\n",
    "len0=len(Xy_train0)\n",
    "\n",
    "\n",
    "Xy_train1_os=Xy_train1.sample(len0,replace=True)\n",
    "Xy_train_os=pd.concat([Xy_train1_os,Xy_train0],axis=0) #bringing togther noramal data and oversample data together\n",
    "\n",
    "y_train_os=Xy_train_os['Class']\n",
    "X_train_os=Xy_train_os.drop('Class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_train_os))   ##we made 1 to match with zeroes. so 2 lakh became 4 lakhs here.there were almost 2 lakh zeroes and 357 '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier(**rand_search_rfc.best_params_)\n",
    "\n",
    "model_compare(rfc,X_train_os,X_test,y_train_os,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### handling imbalance data-SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote=SMOTE(ratio='minority')\n",
    "X_train_sm,y_train_sm=smote.fit_sample(X_train,y_train)\n",
    "model_compare(rfc,X_train_sm,X_test,y_train_sm,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking whether sample is as same as population\n",
    "-a.\tAre both train and test representative of the overall data? How would you ascertain this statistically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one sample t test:\n",
    "\n",
    "#we shall use one sample t test to check the hypothesis:\n",
    "#fi nd mean and std deviation of the target variable from dataframe\n",
    "print('Mean is %2.1f Sd is %2.1f' % (df['Churn'].mean(),np.std(df['Churn'],ddof = 1)))\n",
    "\n",
    "from scipy.stats             import ttest_1samp,ttest_ind, wilcoxon\n",
    "from statsmodels.stats.power import ttest_power\n",
    "\n",
    "\n",
    "#first check y_train with above mean\n",
    "t_statistic, p_value = ttest_1samp(y_train, df['churn'].mean())\n",
    "print(t_statistic, p_value)\n",
    "\n",
    "#second, check y_test with above mean\n",
    "t_statistic, p_value = ttest_1samp(y_test, df['churn'].mean())\n",
    "print(t_statistic, p_value)\n",
    "\n",
    "#explain about hypothesis. \n",
    "#ho: means of sample and population are same.\n",
    "#h1: means are different. we need h0 to be accpeted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skeleton for classification  probelm:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) Read Data\n",
    "2) Shape\n",
    "3)identify categorical and numerical features.\n",
    "        a) write inference with proportion of categorical data.\n",
    "3) Target variable imbalance (only in train set. apply model in test(test should be as is)\n",
    "        a)undersampling majority class\n",
    "        b)oversampling minority class\n",
    "        c)SMOTE-synthetic minority oversampling technique\n",
    "4) Null value imputation:\n",
    "        a)if missing value less than 10% use median/mean\n",
    "        b)more than 10%,bfill/ffill. (but it cant be used in cross sectional data where autocorrelation is zero)\n",
    "        c) more than 70%, make it into dummy variables(1-no miss, 0-miss data)\n",
    "        d) if target has missing values, drop it.\n",
    "5) 5 point summary (describe())\n",
    "        a) atleast write of two points\n",
    "6) plots/graphs\n",
    "        a)Important - bivariant analysis (target vs feature)\n",
    "        b) univariate analysis\n",
    "        c) correaltion- heatmap\n",
    "7) outlier detection and removal:\n",
    "        a) use iqr method ( if outlier are more, mean will be already compromised. hence, a score method wont be effective)\n",
    "        b)in regression, if target has outlier, dont remove it. instead transform the target variable to include outlier.\n",
    "8) categorical into dummies\n",
    "        a)dont convert target into dummies\n",
    "        b)if range of categorical columns are less(<=5), convert it into categorical(even if it is float/int). but only ater             understanding the data.\n",
    "        c)if range of categorical are high(>6), keep it as is.\n",
    "        d)always drop_first=True\n",
    "9) Train and test split\n",
    "        a)make data into standardized(z score) when algorithm has distance calcualtion in it.min_max method(normalization) is             only used in artificial neural network.\n",
    "        b)use 1 sample t test to check whether sample mean of test is same as train. ( sample vs population)\n",
    "        c)check 5 point summary of both test and train with inference\n",
    "10) Base model\n",
    "        a) statistical approach. Logistica regression is preferred mostly though\n",
    "        b)improve model by vif/feature selection/feature engineering (poly)\n",
    "11) machine learning techniques:\n",
    "        a) linear models can use feature selection (vif). ex: logistic and svm\n",
    "        b)improve model by \n",
    "                            a.1)hyperparamter tuning\n",
    "                            a.2)interaction/feature engineering/PCA\n",
    "        c)to get full effect of boarder line data, binning can be done and make it new variable.\n",
    "        d) regularization for overfitting/underfitting.\n",
    "12) Mapping all results and picking up best algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "def function(a,b,c,d,A,B,C,D):\n",
    "    Index=['RMSE','R^2','Adj R^2','Data size']\n",
    "    data={'Train':[a,b,c,d], 'Test':[A,B,C,D]}\n",
    "    df3=DataFrame(data,columns=['Train','Test'],index=Index)\n",
    "    print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for train\n",
    "a=rmse1\n",
    "b=r21\n",
    "c=model_1.rsquared_adj\n",
    "d=X_train.size\n",
    "\n",
    "#for test\n",
    "A=np.sqrt(MSE)\n",
    "B=r2\n",
    "C=model_11.rsquared_adj\n",
    "D=X_test.size\n",
    "\n",
    "\n",
    "function(rmse1,r21,model_1.rsquared_adj,X_train.size,np.sqrt(MSE),r2,model_11.rsquared_adj,X_test.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when we get \n",
    "linalgerror singular matrix\n",
    "\n",
    "do\n",
    "result=model.fit(method='bfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to silent the warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
